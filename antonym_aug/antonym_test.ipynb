{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Specific Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import *\n",
    "\n",
    "from methods import *\n",
    "from b_2_train_eval import run_model\n",
    "from collections import defaultdict\n",
    "from antonym_aug import AR, eda_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = 'data/subj/train_orig.txt'\n",
    "train_aug_st = 'data/subj/train_aug_st.txt'\n",
    "train_aug_ar = 'data/subj/train_aug_ar.txt'\n",
    "train_aug_eda_ar = 'data/subj/train_aug_eda_ar.txt'\n",
    "test_path = 'data/subj/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ar_aug(traing_orig, output_file, alpha=0.3, num_aug=9):\n",
    "    writer = open(output_file, 'w')\n",
    "    lines = open(train_orig, 'r').readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line[:-1].split('\\t')\n",
    "        label = parts[0]\n",
    "        sentence = parts[1]\n",
    "        aug_sentences = AR(sentence, alpha=alpha, num_aug=num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            writer.write(label + '\\t' + aug_sentence + '\\n')\n",
    "    writer.close()\n",
    "    print('finished AR for', train_orig, 'to', output_file, 'with alpha', alpha)\n",
    "\n",
    "def gen_eda_ar_aug(traing_orig, output_file, num_aug=9):\n",
    "    writer = open(output_file, 'w')\n",
    "    lines = open(train_orig, 'r').readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line[:-1].split('\\t')\n",
    "        label = parts[0]\n",
    "        sentence = parts[1]\n",
    "        aug_sentences = eda_5(sentence, num_aug=num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            writer.write(label + '\\t' + aug_sentence + '\\n')\n",
    "    writer.close()\n",
    "    print('finished EDA + AR for', train_orig, 'to', output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_standard_aug(train_orig, train_aug_st, num_aug=1)\n",
    "gen_ar_aug(train_orig, train_aug_ar, alpha=0.3, num_aug=1)\n",
    "gen_eda_ar_aug(train_orig, train_aug_eda_ar, num_aug=1)\n",
    "\n",
    "word2vec_pickle = 'data/subj/word2vec.p'\n",
    "gen_vocab_dicts('data/subj', word2vec_pickle, '../../eda_nlp/word2vec/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $train_aug_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_accs = {}\n",
    "aug_accs = {}\n",
    "ar_accs = {}\n",
    "eda_ar_accs = {}\n",
    "word2vec = load_pickle(word2vec_pickle)\n",
    "num_classes = 2\n",
    "input_size = 40\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_file, test_file, num_classes, percent_dataset):\n",
    "\n",
    "\t#initialize model\n",
    "\tmodel = build_model(input_size, word2vec_len, num_classes)\n",
    "\n",
    "\t#load data\n",
    "\ttrain_x, train_y = get_x_y(train_file, num_classes, word2vec_len, input_size, word2vec, percent_dataset)\n",
    "\ttest_x, test_y = get_x_y(test_file, num_classes, word2vec_len, input_size, word2vec, 1)\n",
    "\n",
    "\t#implement early stopping\n",
    "\tcallbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "\t#train model\n",
    "\tmodel.fit(\ttrain_x, \n",
    "\t\t\t\ttrain_y, \n",
    "\t\t\t\tepochs=100000, \n",
    "\t\t\t\tcallbacks=callbacks,\n",
    "\t\t\t\tvalidation_split=0.1, \n",
    "\t\t\t\tbatch_size=1024, \n",
    "\t\t\t\tshuffle=True, \n",
    "\t\t\t\tverbose=0)\n",
    "\t#model.save('checkpoints/lol')\n",
    "\t#model = load_model('checkpoints/lol')\n",
    "\n",
    "\t#evaluate model\n",
    "\ty_pred = model.predict(test_x)\n",
    "\ttest_y_cat = one_hot_to_categorical(test_y)\n",
    "\ty_pred_cat = one_hot_to_categorical(y_pred)\n",
    "\tacc = accuracy_score(test_y_cat, y_pred_cat)\n",
    "\n",
    "\t#clean memory???\n",
    "\ttrain_x, train_y = None, None\n",
    "\tgc.collect()\n",
    "\n",
    "\t#return the accuracy\n",
    "\t#print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
    "\treturn acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increments = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "for increment in increments:\n",
    "\t\t\t\n",
    "    #calculate augmented accuracy\n",
    "    aug_acc = run_model(train_aug_st, test_path, num_classes, increment)\n",
    "    aug_accs[increment] = aug_acc\n",
    "\n",
    "    #calculate AR accuracy\n",
    "    ar_acc = run_model(train_aug_ar, test_path, num_classes, increment)\n",
    "    ar_accs[increment] = ar_acc\n",
    "\n",
    "    #calculate EDA + AR accuracy\n",
    "    eda_ar_acc = run_model(train_aug_eda_ar, test_path, num_classes, increment)\n",
    "    eda_ar_accs[increment] = eda_ar_acc\n",
    "\n",
    "\n",
    "    print(increment, aug_acc, ar_acc, eda_ar_accs)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [100*p for p in aug_accs.keys()]\n",
    "y1 = list(aug_accs.values())\n",
    "y2 = list(ar_accs.values())\n",
    "y3 = list(eda_ar_accs.values())\n",
    "\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.plot(x, y3)\n",
    "plt.ylim(0.4, 1)\n",
    "plt.legend(['EDA', 'AR', 'EDA+AR'])\n",
    "plt.xlabel('Percent of Dataset (%)')\n",
    "plt.ylabel('Acuracy')\n",
    "plt.savefig('img/eda-ar-result.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbddaa3b4e8f962217deb35e61991761fc8f388ec08d996c8132db1e00a2f7b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
